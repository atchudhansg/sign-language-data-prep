{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385506ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full weights loaded into triplet model\n",
      "✅ Embedding model ready for single-gesture inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# === Embedding Model Definition ===\n",
    "def create_embedding_model(input_shape):\n",
    "    input_layer = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(input_layer)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# === Triplet Loss (Only used for original training, not needed during loading) ===\n",
    "def triplet_loss(alpha=0.2):\n",
    "    def loss(y_true, y_pred):\n",
    "        anchor, positive, negative = y_pred[:, :128], y_pred[:, 128:256], y_pred[:, 256:]\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        return tf.reduce_mean(tf.maximum(pos_dist - neg_dist + alpha, 0.0))\n",
    "    return loss\n",
    "\n",
    "# === Build Triplet Model and Load Full Weights ===\n",
    "def create_triplet_model(input_shape):\n",
    "    embedding_model = create_embedding_model(input_shape)\n",
    "\n",
    "    anchor_input = tf.keras.Input(shape=input_shape)\n",
    "    positive_input = tf.keras.Input(shape=input_shape)\n",
    "    negative_input = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    anchor_embedding = embedding_model(anchor_input)\n",
    "    positive_embedding = embedding_model(positive_input)\n",
    "    negative_embedding = embedding_model(negative_input)\n",
    "\n",
    "    concatenated = tf.keras.layers.Concatenate()([\n",
    "        anchor_embedding, positive_embedding, negative_embedding\n",
    "    ])\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[anchor_input, positive_input, negative_input],\n",
    "        outputs=concatenated\n",
    "    )\n",
    "    model.compile(optimizer='adam', loss=triplet_loss())\n",
    "    return model, embedding_model\n",
    "\n",
    "# === Load Full Weights, Extract Embedding Model ===\n",
    "input_shape = (60, 126)\n",
    "\n",
    "triplet_model, embedding_model = create_triplet_model(input_shape)\n",
    "triplet_model.load_weights(\"/Users/atchudhansreekanth/Downloads/similarity.weights.h5\")\n",
    "print(\"✅ Full weights loaded into triplet model\")\n",
    "\n",
    "# Now embedding_model is ready to use\n",
    "print(\"✅ Embedding model ready for single-gesture inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73793b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label encoder loaded.\n",
      "✅ Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# === Load Label Encoder ===\n",
    "label_encoder = joblib.load(\"/Users/atchudhansreekanth/Downloads/label_encoder.pkl\")\n",
    "print(\"✅ Label encoder loaded.\")\n",
    "\n",
    "# === Redefine Custom Layer Used in Model ===\n",
    "class TemporalPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        d_model = inputs.shape[-1]\n",
    "        position = tf.cast(tf.range(seq_len)[:, tf.newaxis], tf.float32)\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / tf.cast(d_model, tf.float32)))\n",
    "        sin_vals = tf.sin(position * div_term)\n",
    "        cos_vals = tf.cos(position * div_term)\n",
    "        pe = tf.concat([sin_vals, cos_vals], axis=-1)\n",
    "        pe = pe[tf.newaxis, :, :]\n",
    "        return inputs + pe\n",
    "\n",
    "# === Load Model with Custom Objects ===\n",
    "classification_model = tf.keras.models.load_model(\n",
    "    \"/Users/atchudhansreekanth/Downloads/gesture_model.keras\",\n",
    "    custom_objects={\"TemporalPositionalEncoding\": TemporalPositionalEncoding}\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edbe2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743185314.841403   59018 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1743185314.852919   68186 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743185314.857362   68186 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Webcam started. Press 'q' to quit.\n",
      "{\n",
      "    \"gesture\": \"None\",\n",
      "    \"confidence\": 0.0,\n",
      "    \"similarity\": 0.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9433,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9427,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9422,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9422,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9422,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9421,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9421,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"hello\",\n",
      "    \"confidence\": 0.9426,\n",
      "    \"similarity\": 1.0\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎥 Webcam started. Press \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to quit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Failed to grab frame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import time\n",
    "import json\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === 2. Setup Mediapipe ===\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, model_complexity=0, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "frame_buffer = collections.deque(maxlen=5)\n",
    "\n",
    "# === 3. Normalize Landmarks ===\n",
    "def normalize_landmarks(landmarks):\n",
    "    try:\n",
    "        points = np.array(landmarks).reshape(21, 2)\n",
    "        base_x, base_y = points[0]\n",
    "        points -= [base_x, base_y]\n",
    "        max_dist = np.linalg.norm(points, axis=1).max()\n",
    "        if max_dist > 0:\n",
    "            points /= max_dist\n",
    "        return points.flatten()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# === 4. Predict Classification & Similarity ===\n",
    "# NOTE: These must be loaded before running this script\n",
    "# classification_model\n",
    "# embedding_model\n",
    "# label_encoder\n",
    "\n",
    "# Add reference embeddings as needed (optional)\n",
    "reference_embeddings = {\n",
    "    # Example (load with np.load): 'hello': np.load('ref_hello.npy')\n",
    "}\n",
    "\n",
    "def predict(frames):\n",
    "    input_seq = np.array(frames).reshape(5, 84)\n",
    "    padded_seq = np.pad(input_seq, ((0, 55), (0, 42)), mode='constant')  # (60, 126)\n",
    "    model_input = np.expand_dims(padded_seq, axis=0)  # (1, 60, 126)\n",
    "\n",
    "    # === Classification\n",
    "    preds = classification_model.predict(model_input, verbose=0)\n",
    "    confidence = float(np.max(preds))\n",
    "    label = label_encoder.inverse_transform([np.argmax(preds)])[0]\n",
    "\n",
    "    # === Embedding\n",
    "    embedding = embedding_model.predict(model_input, verbose=0)[0]\n",
    "\n",
    "    # === Similarity with Reference (if available)\n",
    "    if label in reference_embeddings:\n",
    "        ref = reference_embeddings[label]\n",
    "        similarity = float(cosine_similarity([embedding], [ref])[0][0])\n",
    "    else:\n",
    "        similarity = float(np.dot(embedding, embedding))  # fallback self-similarity\n",
    "\n",
    "    return label, confidence, similarity\n",
    "\n",
    "\n",
    "# === 5. Start Webcam ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"❌ Could not access webcam.\")\n",
    "\n",
    "last_prediction = {\"gesture\": \"None\", \"confidence\": 0.0, \"similarity\": 0.0}\n",
    "last_print_time = time.time()\n",
    "PRINT_INTERVAL = 2\n",
    "\n",
    "print(\"🎥 Webcam started. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠️ Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        combined_hand_data = []\n",
    "        for hand in result.multi_hand_landmarks:\n",
    "            lm = [(pt.x, pt.y) for pt in hand.landmark]\n",
    "            norm = normalize_landmarks(lm)\n",
    "            if norm is not None:\n",
    "                combined_hand_data.extend(norm)\n",
    "            mp_draw.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if len(combined_hand_data) == 84:\n",
    "            frame_buffer.append(combined_hand_data)\n",
    "        elif len(result.multi_hand_landmarks) == 1 and len(combined_hand_data) == 42:\n",
    "            combined_hand_data.extend([0.0] * 42)\n",
    "            frame_buffer.append(combined_hand_data)\n",
    "\n",
    "        if len(frame_buffer) == 5:\n",
    "            gesture, conf, sim = predict(frame_buffer)\n",
    "\n",
    "            # Only update if gesture or confidence changes significantly\n",
    "            significant_change = (\n",
    "                gesture != last_prediction[\"gesture\"] or\n",
    "                abs(conf - last_prediction[\"confidence\"]) > 0.02 or\n",
    "                abs(sim - last_prediction[\"similarity\"]) > 0.05\n",
    "            )\n",
    "\n",
    "            time_elapsed = time.time() - last_print_time >= PRINT_INTERVAL\n",
    "\n",
    "            if significant_change or time_elapsed:\n",
    "                last_prediction = {\n",
    "                    \"gesture\": gesture,\n",
    "                    \"confidence\": round(conf, 4),\n",
    "                    \"similarity\": round(sim, 4)\n",
    "                }\n",
    "\n",
    "                # Print to console\n",
    "                print(json.dumps(last_prediction, indent=4))\n",
    "                last_print_time = time.time()\n",
    "\n",
    "\n",
    "    # === Display Info ===\n",
    "    label = last_prediction[\"gesture\"]\n",
    "    conf = last_prediction[\"confidence\"]\n",
    "    sim = last_prediction[\"similarity\"]\n",
    "    cv2.putText(frame, f\"{label} ({conf*100:.1f}%, Sim: {sim*100:.1f}%)\", (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    if time.time() - last_print_time >= PRINT_INTERVAL:\n",
    "        print(json.dumps(last_prediction, indent=4))\n",
    "        last_print_time = time.time()\n",
    "\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e1887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
