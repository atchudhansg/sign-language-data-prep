{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385506ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and encoder loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743003828.064798  150152 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743003828.091318  150721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743003828.098147  150721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Webcam started. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:13:50.011 python[2788:150152] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-03-26 21:13:50.011 python[2788:150152] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "W0000 00:00:1743003830.864212  150721 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743003831.616416  150712 service.cc:152] XLA service 0x600002571e00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743003831.616430  150712 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1743003832.004072  150712 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gesture\": \"BORED\",\n",
      "    \"confidence\": 0.9551\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.8237\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"ABUSE\",\n",
      "    \"confidence\": 0.8614\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"ABUSE\",\n",
      "    \"confidence\": 0.6685\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.9936\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.8775\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"ANGRY\",\n",
      "    \"confidence\": 0.9103\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.7348\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.9334\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"ABUSE\",\n",
      "    \"confidence\": 0.573\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.9997\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"BORED\",\n",
      "    \"confidence\": 0.8385\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"COLLEGE_SCHOOL\",\n",
      "    \"confidence\": 0.8216\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AFRAID\",\n",
      "    \"confidence\": 0.7745\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AFRAID\",\n",
      "    \"confidence\": 0.7007\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AFRAID\",\n",
      "    \"confidence\": 0.7007\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AFRAID\",\n",
      "    \"confidence\": 0.7007\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"BORED\",\n",
      "    \"confidence\": 0.7027\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"CLASS\",\n",
      "    \"confidence\": 0.7855\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AGREE\",\n",
      "    \"confidence\": 0.9689\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AGREE\",\n",
      "    \"confidence\": 0.9807\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"CLASS\",\n",
      "    \"confidence\": 0.6857\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"BORED\",\n",
      "    \"confidence\": 0.7513\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AGREE\",\n",
      "    \"confidence\": 0.5702\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AGREE\",\n",
      "    \"confidence\": 0.5702\n",
      "}\n",
      "{\n",
      "    \"gesture\": \"AGREE\",\n",
      "    \"confidence\": 0.5702\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(frame, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 68\u001b[0m result \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(rgb)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hand \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"false\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_MLIR_ENABLE_METAL\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import time\n",
    "import json\n",
    "import collections\n",
    "\n",
    "# === 1. Load model + encoder ===\n",
    "model = tf.keras.models.load_model('/Users/atchudhansreekanth/Desktop/University/Proj/Dataset prep/gesture_transformer_model.keras')\n",
    "label_encoder = joblib.load('/Users/atchudhansreekanth/Desktop/University/Proj/Dataset prep/label_encoder.pkl')\n",
    "print(\"✅ Model and encoder loaded.\")\n",
    "\n",
    "# === 2. Setup Mediapipe ===\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, model_complexity=0, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "frame_buffer = collections.deque(maxlen=5)\n",
    "\n",
    "# === 3. Normalize ===\n",
    "def normalize_landmarks(landmarks):\n",
    "    try:\n",
    "        points = np.array(landmarks).reshape(21, 2)\n",
    "        base_x, base_y = points[0]\n",
    "        points -= [base_x, base_y]\n",
    "        max_dist = np.linalg.norm(points, axis=1).max()\n",
    "        if max_dist > 0:\n",
    "            points /= max_dist\n",
    "        return points.flatten()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# === 4. Predict Gesture ===\n",
    "def predict_gesture(frames):\n",
    "    input_data = np.array(frames).reshape(1, 5, 42)\n",
    "    preds = model.predict(input_data, verbose=0)\n",
    "    confidence = float(np.max(preds))\n",
    "    label = label_encoder.inverse_transform([np.argmax(preds)])[0]\n",
    "    return label, confidence\n",
    "\n",
    "# === 5. Start Webcam ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"❌ Could not access webcam.\")\n",
    "\n",
    "last_prediction = {\"gesture\": \"None\", \"confidence\": 0.0}\n",
    "last_print_time = time.time()\n",
    "PRINT_INTERVAL = 2  # seconds\n",
    "\n",
    "print(\"🎥 Webcam started. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠️ Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand in result.multi_hand_landmarks:\n",
    "            lm = [(pt.x, pt.y) for pt in hand.landmark]\n",
    "            norm = normalize_landmarks(lm)\n",
    "            if norm is not None:\n",
    "                frame_buffer.append(norm)\n",
    "\n",
    "            if len(frame_buffer) == 5:\n",
    "                try:\n",
    "                    gesture, conf = predict_gesture(frame_buffer)\n",
    "                    last_prediction = {\"gesture\": gesture, \"confidence\": round(conf, 4)}\n",
    "                except Exception as e:\n",
    "                    print(\"❌ Prediction error:\", e)\n",
    "\n",
    "            mp_draw.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display prediction on screen\n",
    "    label = last_prediction[\"gesture\"]\n",
    "    conf = last_prediction[\"confidence\"]\n",
    "    cv2.putText(frame, f\"{label} ({conf*100:.1f}%)\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Print to console every few seconds\n",
    "    if time.time() - last_print_time >= PRINT_INTERVAL:\n",
    "        print(json.dumps(last_prediction, indent=4))\n",
    "        last_print_time = time.time()\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
