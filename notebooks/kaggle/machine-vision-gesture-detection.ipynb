{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11163542,"sourceType":"datasetVersion","datasetId":6966179},{"sourceId":11168429,"sourceType":"datasetVersion","datasetId":6969592},{"sourceId":255541,"sourceType":"modelInstanceVersion","modelInstanceId":218464,"modelId":240191},{"sourceId":258936,"sourceType":"modelInstanceVersion","modelInstanceId":221330,"modelId":243110}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install mediapipe opencv-python pandas scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:05:01.904922Z","iopub.execute_input":"2025-03-26T04:05:01.905127Z","iopub.status.idle":"2025-03-26T04:05:18.366565Z","shell.execute_reply.started":"2025-03-26T04:05:01.905107Z","shell.execute_reply":"2025-03-26T04:05:18.365577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#preprocessing code\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# ðŸ”¹ Configurable Parameters\nINPUT_CSV = \"/kaggle/input/data-v1/data.csv\"\nOUTPUT_CSV = \"gesture_all_balanced_augmented.csv\"\nAUGMENT_NOISE = 0.01        # How much random noise to add to landmarks\nMIN_SAMPLES_PER_CLASS = 30  # Desired minimum per gesture\nTIME_STEPS = 5              # For sequence creation\n\n# ðŸ”¹ Step 1: Load Full Dataset\ndf = pd.read_csv(INPUT_CSV)\nprint(f\"ðŸ“¥ Loaded dataset with {len(df)} samples and {df['gesture'].nunique()} classes\")\n\n# ðŸ”¹ Step 2: Count Samples Per Gesture\nclass_counts = Counter(df[\"gesture\"])\nmax_dim = df.shape[1] - 1\n\n# ðŸ”¹ Step 3: Augment Underrepresented Gestures\naugmented_rows = []\n\nfor gesture, count in class_counts.items():\n    if count >= MIN_SAMPLES_PER_CLASS:\n        continue  # Enough samples\n\n    num_needed = MIN_SAMPLES_PER_CLASS - count\n    samples = df[df[\"gesture\"] == gesture]\n\n    for _ in range(num_needed):\n        row = samples.sample(n=1).iloc[0]\n        landmark = row.iloc[1:].values.astype(np.float32)\n        noise = np.random.normal(0, AUGMENT_NOISE, size=landmark.shape)\n        augmented = landmark + noise\n        augmented_rows.append([gesture] + list(augmented))\n\n# ðŸ”¹ Combine Original + Augmented\ndf_aug = pd.DataFrame(augmented_rows, columns=df.columns)\ndf_final = pd.concat([df, df_aug], ignore_index=True)\ndf_final.to_csv(OUTPUT_CSV, index=False)\nprint(f\"âœ… Saved full + balanced dataset: {OUTPUT_CSV} ({len(df_final)} samples)\")\n\n# ðŸ”¹ Step 4: Prepare for Transformer (X_seq, y_seq)\nX = df_final.iloc[:, 1:].values\ny_raw = df_final[\"gesture\"].values\n\n# Label Encoding\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_raw)\n\n# Sequence Preparation\nX_seq = np.array([X[i:i + TIME_STEPS] for i in range(len(X) - TIME_STEPS)])\ny_seq = y_encoded[TIME_STEPS:]\n\n# One-Hot Encoding\none_hot_encoder = OneHotEncoder(sparse_output=False)\ny_seq = one_hot_encoder.fit_transform(y_seq.reshape(-1, 1))\n\nprint(f\"âœ… Final Tensor Shapes â€” X: {X_seq.shape}, y: {y_seq.shape}\")\nprint(f\"ðŸ§¾ Total Classes: {len(label_encoder.classes_)} | Labels: {list(label_encoder.classes_)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:20:19.927399Z","iopub.execute_input":"2025-03-26T04:20:19.927749Z","iopub.status.idle":"2025-03-26T04:20:20.897829Z","shell.execute_reply.started":"2025-03-26T04:20:19.927722Z","shell.execute_reply":"2025-03-26T04:20:20.896981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib  # To save encoders\n\n# ðŸ”¹ Load Preprocessed Dataset\ndf = pd.read_csv(\"/kaggle/working/gesture_balanced.csv\")\n\n# ðŸ”¹ Prepare Features and Labels\nX = df.iloc[:, 1:].values\ny_raw = df[\"gesture\"].values\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_raw)\n\n# Save label encoder for later inference\njoblib.dump(label_encoder, \"label_encoder.pkl\")\n\n# Create time-windowed sequences\ntime_steps = 5\nX_seq = np.array([X[i:i + time_steps] for i in range(len(X) - time_steps)])\ny_seq = y_encoded[time_steps:]\n\n# One-hot encode the labels\none_hot_encoder = OneHotEncoder(sparse_output=False)\ny_seq = one_hot_encoder.fit_transform(y_seq.reshape(-1, 1))\n\n# Save one-hot encoder for inference if needed\njoblib.dump(one_hot_encoder, \"one_hot_encoder.pkl\")\n\n# Get number of classes\nnum_classes = y_seq.shape[1]\n\n# ðŸ”¹ Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n)\n\n# ðŸ”¹ Transformer Model Definition\nfrom tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D, BatchNormalization\nfrom tensorflow.keras.models import Model\n\ndef build_simple_transformer(num_classes, time_steps=5, feature_dim=42):\n    \"\"\"\n    A compact Transformer model for small gesture datasets.\n    \"\"\"\n    inputs = Input(shape=(time_steps, feature_dim))\n\n    # ðŸ”¹ Lightweight Attention\n    attn = MultiHeadAttention(num_heads=2, key_dim=16)(inputs, inputs)\n    attn = LayerNormalization()(attn)\n    attn = Dropout(0.2)(attn)\n\n    # ðŸ”¹ Global Pooling â†’ Dense\n    x = GlobalAveragePooling1D()(attn)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.2)(x)\n\n    # ðŸ”¹ Output\n    outputs = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# ðŸ”¹ Build & Train the Model\nmodel = build_simple_transformer(num_classes)\nmodel.summary()\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=100,\n    batch_size=32\n)\n\n# ðŸ”¹ Save Trained Model\nmodel.save(\"gesture_transformer_model.keras\")\nprint(\"âœ… Model training complete and saved as gesture_transformer_model.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:27:25.357739Z","iopub.execute_input":"2025-03-26T04:27:25.358055Z","iopub.status.idle":"2025-03-26T04:27:41.502022Z","shell.execute_reply.started":"2025-03-26T04:27:25.358034Z","shell.execute_reply":"2025-03-26T04:27:41.501275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\nprint(Counter(df[\"gesture\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:16:13.880846Z","iopub.execute_input":"2025-03-26T04:16:13.881152Z","iopub.status.idle":"2025-03-26T04:16:13.885864Z","shell.execute_reply.started":"2025-03-26T04:16:13.881126Z","shell.execute_reply":"2025-03-26T04:16:13.885117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#removing class imbalance\nfrom collections import Counter\n\nmin_samples = 10\ncounts = Counter(df[\"gesture\"])\nvalid_classes = [cls for cls, count in counts.items() if count >= min_samples]\n\ndf_balanced = df[df[\"gesture\"].isin(valid_classes)].reset_index(drop=True)\ndf_balanced.to_csv(\"gesture_balanced.csv\", index=False)\n\nprint(f\"âœ… Using {len(valid_classes)} balanced classes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:17:40.974473Z","iopub.execute_input":"2025-03-26T04:17:40.974777Z","iopub.status.idle":"2025-03-26T04:17:41.016808Z","shell.execute_reply.started":"2025-03-26T04:17:40.974754Z","shell.execute_reply":"2025-03-26T04:17:41.016092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:56:43.938992Z","iopub.execute_input":"2025-03-26T14:56:43.939432Z","iopub.status.idle":"2025-03-26T14:56:44.064769Z","shell.execute_reply.started":"2025-03-26T14:56:43.939402Z","shell.execute_reply":"2025-03-26T14:56:44.063354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cloning the repository\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\ntoken = user_secrets.get_secret(\"PAT\")\n\n!git clone https://{token}@github.com/atchudhansg/sign-language-data-prep.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:53:25.265078Z","iopub.execute_input":"2025-03-26T14:53:25.265514Z","iopub.status.idle":"2025-03-26T14:53:26.289478Z","shell.execute_reply.started":"2025-03-26T14:53:25.265485Z","shell.execute_reply":"2025-03-26T14:53:26.288285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\n\n#github authentication \n!git config --global user.name \"atchudhansg\"\n!git config --global user.email \"116624804+atchudhansg@users.noreply.github.com\"\n\nuser_secrets = UserSecretsClient()\nkaggle_username = user_secrets.get_secret(\"KAGGLE_USERNAME\")\nkaggle_key = user_secrets.get_secret(\"KAGGLE_KEY\")\n\n# âœ… Set environment variables so the Kaggle API can find them\nos.environ[\"KAGGLE_USERNAME\"] = kaggle_username\nos.environ[\"KAGGLE_KEY\"] = kaggle_key\n\n#download the notebook in working directory\n!kaggle kernels pull atchusg/machine-vision-gesture-detection -p /kaggle/working\n\n#check if the notebook is downloaded or not\n!ls /kaggle/working\n\n!mkdir -p /kaggle/working/sign-language-data-prep/notebooks/kaggle\n!mv /kaggle/working/machine-vision-gesture-detection.ipynb /kaggle/working/sign-language-data-prep/notebooks/kaggle/\n%cd /kaggle/working/sign-language-data-prep\n\n#commit the stages\n!git add .\n!git commit -m \"Update Kaggle Notebook\"\n\n#push to github\ntoken = user_secrets.get_secret(\"PAT\")\n!git push https://{token}@github.com/atchudhansg/sign-language-data-prep.git main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T15:04:14.016935Z","iopub.execute_input":"2025-03-26T15:04:14.017348Z","iopub.status.idle":"2025-03-26T15:04:16.348915Z","shell.execute_reply.started":"2025-03-26T15:04:14.017317Z","shell.execute_reply":"2025-03-26T15:04:16.347398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}